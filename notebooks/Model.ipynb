{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9e5c6b",
   "metadata": {},
   "source": [
    "# Transmissibility of different variants in Chile\n",
    "\n",
    "We want to infer the difference of transmissibility of the different variants currently circulating in Chile. Please have at least a short look into our publication before continuing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48997199",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Make sure to install our \n",
    "`covid19_inference` package or inititialize the\n",
    "submodules `git submodule update --init`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter,WeekdayLocator\n",
    "import pickle\n",
    "import os\n",
    "import arviz as az\n",
    "\n",
    "sys.path.append(\"../covid19_inference\")\n",
    "import covid19_inference as cov19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabceb3",
   "metadata": {},
   "source": [
    "Additionaly we set some environmental variables here, make sure to set `n_processes` according to the cores of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threads = str(1) # per process\n",
    "n_processes = 8\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = n_threads\n",
    "os.environ[\"OMP_NUM_THREADS\"] = n_threads\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = n_threads\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = n_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93102dc2",
   "metadata": {},
   "source": [
    "### Preprocess data\n",
    "\n",
    "We start by loading the variant data for chile i.e. $y_{v,t}$ and $n_{v,t}$. The resolution for the data is weekly, starting on mondays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_end = datetime.datetime(2021,7,24)\n",
    "\n",
    "# Load data variants\n",
    "variants = pd.read_excel(\"../data/Chile_Variants_Updated_with_airports.xlsx\",sheet_name=\"Variants_Count\")\n",
    "variants = variants.set_index('Lineage').T\n",
    "variants.index.name = \"Week\"\n",
    "variants.index = pd.to_datetime(variants.index + '-1', format='%V_%G-%u')\n",
    "variants = variants.iloc[0:-1]\n",
    "variant_names = [\"B.1.1\", \"B.1.1.348\", \"B.1.1.7\", \"C.37\", \"P.1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a45395",
   "metadata": {},
   "source": [
    "Additionaly we download the new confirmed cases in chile with our previous developed download utils. You can find the documentation for the data retrieval module [here](https://covid19-inference.readthedocs.io/en/latest/doc/data_retrieval.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load casenumbers chile and sum over weeks\n",
    "jhu = cov19.data_retrieval.JHU(True)\n",
    "new_cases_obs = jhu.get_new(\n",
    "    country=\"Chile\",\n",
    "    data_begin=variants.index[0], #starting with the same date as the variant data\n",
    "    data_end=data_end\n",
    ")\n",
    "jhu.download_all_available_data(force_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d4cf7",
   "metadata": {},
   "source": [
    "Let us take a short look into the data. We plot the total number of sampled pcr test such as the number of test which could be per variant and the reported cases in chile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a00b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1,figsize=(10,8))\n",
    "\n",
    "s=0\n",
    "for c in variants.columns:\n",
    "    if c == \"N_Total\":\n",
    "        continue\n",
    "    axes[0].bar(variants.index,variants[c],width=3,label=c,bottom=s)\n",
    "    s += variants[c]\n",
    "axes[0] = cov19.plot._timeseries(\n",
    "    x=variants.index,\n",
    "    y=variants[\"N_Total\"],\n",
    "    what=\"model\",\n",
    "    color=\"black\",\n",
    "    ax=axes[0],\n",
    "    label=\"Total\",\n",
    "    lw=2\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[1] = cov19.plot._timeseries(\n",
    "    x=new_cases_obs.index,\n",
    "    y=new_cases_obs,\n",
    "    what=\"model\",\n",
    "    color=\"tab:blue\",\n",
    "    ax=axes[1],\n",
    ")\n",
    "\n",
    "# Date layout\n",
    "date_form = DateFormatter(\"%m-%V\")\n",
    "\n",
    "# Markup\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticks(), rotation=30)\n",
    "    ax.xaxis.set_major_formatter(date_form)\n",
    "    ax.xaxis.set_major_locator(WeekdayLocator(interval=1))\n",
    "    _ = ax.set_xlim(variants.index[0]-datetime.timedelta(days=2), variants.index[-1]+datetime.timedelta(days=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23227278",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "We defined different models and tested which one fits the dynamics best, have a look into `run_model.py` for more informations. For now we are running the default model with dirichlet likelihood and kernelized spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src\")\n",
    "from run_model import create_model\n",
    "\n",
    "likelihood=\"dirichlet\"\n",
    "spreading_dynamics=\"kernelized_spread\"\n",
    "influx_factor=0.2\n",
    "\n",
    "this_model = create_model(\n",
    "    likelihood=likelihood,\n",
    "    spreading_dynamics=spreading_dynamics,\n",
    "    variants=variants,\n",
    "    new_cases_obs=new_cases_obs,\n",
    "    factor_influx=influx_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e6d9b",
   "metadata": {},
   "source": [
    "## Sampling \n",
    "\n",
    "We use our own sampling function which we developed for because of computation time reasons and parallelization on bigger computing clusters. You can also use the default `pm.sample` function which may be faster in envorionments without a lot of cores. Depending on how the draws, tune and chains it can take some time. Additionaly we also supply a small trace in the data folder which can be loaded (see below).\n",
    "\n",
    "```python\n",
    "import pymc3 as pm\n",
    "\n",
    "trace = pm.sample(\n",
    "    model=this_model,\n",
    "    return_inferencedata=True,\n",
    "    cores=n_processes,\n",
    "    chains=4,\n",
    "    draws=2000,\n",
    "    tune=2000,\n",
    "    init=\"advi+adapt_diag\",\n",
    "    target_accept=0.97,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387c334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multitrace, trace, multitrace_tuning, trace_tuning = cov19.robust_sample(\n",
    "    this_model,\n",
    "    draws=1500,\n",
    "    tune=600,\n",
    "    tune_2nd=1200,\n",
    "    tuning_chains=32,\n",
    "    final_chains=16,\n",
    "    return_tuning=True,\n",
    "    max_treedepth=10,\n",
    "    target_accept=0.8,\n",
    "    cores=n_processes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce99f59",
   "metadata": {},
   "source": [
    "Next we save the model and trace, such that we can reuse it at a later point in time, e.g. for the plotting script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trace (maybe we want to load it at a later point)\n",
    "fstring = f\"../data/pickled/Variants-likelihood={likelihood}-spread_method={spreading_dynamics}-influx={influx_factor}_forecast.pickle\"\n",
    "with open(fstring, 'wb') as f:\n",
    "    pickle.dump((this_model,trace),f)\n",
    "print(f\"Saved model trace combo to: '{fstring}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b9519",
   "metadata": {},
   "source": [
    "## Plotting and results\n",
    "\n",
    "To recreate the same plots from our publication please take a look into the `create_plots.ipynb` notebook. In the following we will just take a short look into the convergence and than show a simple plot for the new cases obs and the transmissibility factors $f_v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04252233",
   "metadata": {},
   "source": [
    "First let's check if the chains converged to the same values. We can do that by computing the rhat statistic, it should be around one for most variables. One can also plot the joint log posterior density for the model, most of the times the divergence of the chains is also visible by eye there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2134a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.rhat(trace).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6339169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#effective sample sizes\n",
    "az.ess(trace).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ac27a",
   "metadata": {},
   "source": [
    "Finally let's create some plots for our results i.e. the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f894ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_to_array(trace, key):\n",
    "    \"\"\"\n",
    "    Convert posterior for a key into a numpy array. Flattens\n",
    "    chain and draw dimensions in the arviz trace.\n",
    "    \"\"\"\n",
    "    var = np.array(trace.posterior[key])\n",
    "    var = var.reshape((var.shape[0]*var.shape[1],) + var.shape[2:])\n",
    "    return var\n",
    "\n",
    "model = this_model\n",
    "\"\"\" Plot lambda and casenumbers\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(3,1,figsize=(9,7), gridspec_kw={'height_ratios': [1, 1, 2]})\n",
    "date_range = pd.date_range(model.sim_begin, model.sim_end)\n",
    "\n",
    "# Plot R if B.1.1.7 would have been dominant (or lambda depending on the model)\n",
    "R = posterior_to_array(trace,\"base_lambda_t\")\n",
    "cov19.plot._timeseries(\n",
    "    x=date_range,\n",
    "    y=R,\n",
    "    what=\"model\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_ylabel(\"effective R\\nif B.1.1.7 would\\nhave been dominant\")\n",
    "\n",
    "# Load posterior samples from trace with the corresonding key\n",
    "R = posterior_to_array(trace,\"base_lambda_t\")\n",
    "f = posterior_to_array(trace,\"f_v\")\n",
    "unkn = posterior_to_array(trace,\"unknown_lambda_t\")\n",
    "new_I = posterior_to_array(trace,\"new_I_tv\")\n",
    "S_t = posterior_to_array(trace,\"S_t\")\n",
    "\n",
    "# Compute some interesting values\n",
    "tau_spread = new_I/np.sum(new_I, axis=-1)[...,None]\n",
    "fact = S_t/model.N_population\n",
    "R_eff = np.exp(np.sum(np.log(R[:,:,None]*f[:,None,:])*fact[...,None]*tau_spread[:,:,:-1] + (np.log(unkn*R)*fact*tau_spread[:,:,-1])[:,:,None], axis=-1))\n",
    "\n",
    "# Plot effective Reproduction number\n",
    "cov19.plot._timeseries(\n",
    "    x=date_range,\n",
    "    y=R_eff,\n",
    "    what=\"model\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_ylabel(\"effective R\")\n",
    "axes[1].plot(date_range, np.ones(len(date_range)), color='grey')\n",
    "\n",
    "# Calculate rolling average new_cases\n",
    "nc = posterior_to_array(trace,\"new_cases\")\n",
    "nc = pd.DataFrame(nc.T,index=pd.date_range(model.sim_begin,model.sim_end))\n",
    "\n",
    "# Plot new cases (real)\n",
    "cov19.plot._timeseries(\n",
    "    x=new_cases_obs.index[7:],\n",
    "    y=new_cases_obs.rolling(7).mean().T[7:],\n",
    "    what=\"data\",\n",
    "    ax=axes[2],\n",
    "    color=\"black\",\n",
    "    markersize=4\n",
    ")\n",
    "\n",
    "# Plot new cases (model)\n",
    "cov19.plot._timeseries(\n",
    "    x=nc.index,\n",
    "    y=nc.rolling(7).mean().T,\n",
    "    what=\"model\",\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_ylabel(\"Number of cases\")\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticks(), rotation=30)\n",
    "    ax.xaxis.set_major_formatter(date_form)\n",
    "    ax.xaxis.set_major_locator(WeekdayLocator(interval=1))\n",
    "    # exclude first 7 days because of the rolling window of cases of 7 days\n",
    "    _ = ax.set_xlim(model.data_begin-datetime.timedelta(days=2), model.data_end+datetime.timedelta(days=2))\n",
    "    \n",
    "axes[1].set_ylim(0.5,2)\n",
    "axes[2].set_ylim(0,10000)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5afc88e-d1e6-46de-92f4-012c685c1f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
